{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK_Application_Scanner.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "w0LcR1pjf_I7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8b_8-z2Xjlz",
        "outputId": "b5884f09-c467-4a64-edbe-2d5276bd5ca4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Package abc is already up-to-date!\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Package alpino is already up-to-date!\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package averaged_perceptron_tagger is already up-to-date!\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package averaged_perceptron_tagger_ru is already up-to-\n",
            "       |       date!\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Package basque_grammars is already up-to-date!\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Package biocreative_ppi is already up-to-date!\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Package book_grammars is already up-to-date!\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Package brown is already up-to-date!\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Package brown_tei is already up-to-date!\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Package cess_cat is already up-to-date!\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Package cess_esp is already up-to-date!\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Package chat80 is already up-to-date!\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Package city_database is already up-to-date!\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Package cmudict is already up-to-date!\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package comparative_sentences is already up-to-date!\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       |   Package comtrans is already up-to-date!\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Package conll2000 is already up-to-date!\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Package conll2002 is already up-to-date!\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       |   Package conll2007 is already up-to-date!\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Package crubadan is already up-to-date!\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Package dependency_treebank is already up-to-date!\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Package dolch is already up-to-date!\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Package europarl_raw is already up-to-date!\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       |   Package extended_omw is already up-to-date!\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Package floresta is already up-to-date!\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Package framenet_v15 is already up-to-date!\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Package framenet_v17 is already up-to-date!\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Package gazetteers is already up-to-date!\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Package genesis is already up-to-date!\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Package gutenberg is already up-to-date!\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Package ieer is already up-to-date!\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Package inaugural is already up-to-date!\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Package indian is already up-to-date!\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       |   Package jeita is already up-to-date!\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Package kimmo is already up-to-date!\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       |   Package knbc is already up-to-date!\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Package large_grammars is already up-to-date!\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Package lin_thesaurus is already up-to-date!\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Package mac_morpho is already up-to-date!\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       |   Package machado is already up-to-date!\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       |   Package masc_tagged is already up-to-date!\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Package maxent_ne_chunker is already up-to-date!\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package maxent_treebank_pos_tagger is already up-to-date!\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Package moses_sample is already up-to-date!\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Package movie_reviews is already up-to-date!\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Package mte_teip5 is already up-to-date!\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Package mwa_ppdb is already up-to-date!\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Package names is already up-to-date!\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       |   Package nombank.1.0 is already up-to-date!\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package nonbreaking_prefixes is already up-to-date!\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Package nps_chat is already up-to-date!\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Package omw is already up-to-date!\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       |   Package omw-1.4 is already up-to-date!\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Package opinion_lexicon is already up-to-date!\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       |   Package panlex_swadesh is already up-to-date!\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Package paradigms is already up-to-date!\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Package pe08 is already up-to-date!\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Package perluniprops is already up-to-date!\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Package pil is already up-to-date!\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Package pl196x is already up-to-date!\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Package porter_test is already up-to-date!\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Package ppattach is already up-to-date!\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Package problem_reports is already up-to-date!\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Package product_reviews_1 is already up-to-date!\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Package product_reviews_2 is already up-to-date!\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       |   Package propbank is already up-to-date!\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Package pros_cons is already up-to-date!\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Package ptb is already up-to-date!\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Package punkt is already up-to-date!\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Package qc is already up-to-date!\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       |   Package reuters is already up-to-date!\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Package rslp is already up-to-date!\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Package rte is already up-to-date!\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Package sample_grammars is already up-to-date!\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       |   Package semcor is already up-to-date!\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Package senseval is already up-to-date!\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Package sentence_polarity is already up-to-date!\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Package sentiwordnet is already up-to-date!\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Package shakespeare is already up-to-date!\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Package sinica_treebank is already up-to-date!\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Package smultron is already up-to-date!\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       |   Package snowball_data is already up-to-date!\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Package spanish_grammars is already up-to-date!\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Package state_union is already up-to-date!\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Package stopwords is already up-to-date!\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Package subjectivity is already up-to-date!\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Package swadesh is already up-to-date!\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Package switchboard is already up-to-date!\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Package tagsets is already up-to-date!\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Package timit is already up-to-date!\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Package toolbox is already up-to-date!\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Package treebank is already up-to-date!\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Package twitter_samples is already up-to-date!\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Package udhr is already up-to-date!\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Package udhr2 is already up-to-date!\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Package unicode_samples is already up-to-date!\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Package universal_tagset is already up-to-date!\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package universal_treebanks_v20 is already up-to-date!\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       |   Package vader_lexicon is already up-to-date!\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Package verbnet is already up-to-date!\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Package verbnet3 is already up-to-date!\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Package webtext is already up-to-date!\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Package wmt15_eval is already up-to-date!\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Package word2vec_sample is already up-to-date!\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Package wordnet is already up-to-date!\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       |   Package wordnet2021 is already up-to-date!\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       |   Package wordnet31 is already up-to-date!\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Package wordnet_ic is already up-to-date!\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Package words is already up-to-date!\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Package ycoe is already up-to-date!\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bakery_job_description = '''\n",
        "Bakery Assistant \n",
        "We are looking for a highly personable and enthusiastic bakery assistant to assist us at our baked goods counter. Your duties will include selling our baked goods to customers, managing baked goods and ingredient inventories, and taking control of the daily cleanup procedures at the bakery. You are also be required to monitor ingredient expiry dates.\n",
        "To ensure success, bakery assistants should exhibit experience in serving customers in a retail setting and a customer-oriented approach. Outstanding candidates are highly organized and have excellent communication skills.\n",
        "Bakery Assistant Responsibilities:\n",
        "Creating a welcoming and positive customer experience at the bakery counter.\n",
        "Advising customers on baked goods selection and taking orders.\n",
        "Weighing, pricing, and packaging purchased items, as well as processing payments.\n",
        "Managing the baking ingredients inventory and requesting the purchase of stock.\n",
        "Tracking ingredient expiry dates and arranging their use accordingly.\n",
        "Monitoring the visual appeal and the availability of baked goods on display.\n",
        "Reporting low baked goods stock to the Baker in a timely manner.\n",
        "Maintaining a clean and tidy baked goods counter and performing other duties on request.\n",
        "Cleaning the kitchen after business hours and preparing it for the next day.\n",
        "Managing customer complaints and relaying them to the Baker.\n",
        "Bakery Assistant Requirements:\n",
        "High school diploma or GED.\n",
        "Vocational training or baking coursework would be advantageous.\n",
        "State-approved food handling permit.\n",
        "Previous experience in serving customers in a bakery environment preferred.\n",
        "Proficiency in weighing, pricing, packaging, and processing payments of baked goods.\n",
        "Ability to manage ingredient inventories and to track expiry dates.\n",
        "A keen eye for detail and the ability to create visually appealing displays of baked goods.\n",
        "Knowledge of food and health industry regulations.\n",
        "Advanced ability to multitask and follow instructions given by bakers.\n",
        "Excellent customer service and communication skills. '''\n"
      ],
      "metadata": {
        "id": "C2U2YMW6l1K0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_job_description = '''  Senior Data Analyst- job post\n",
        "Southwest Texas Regional Advisory Council\n",
        "San Antonio, TX 78227\n",
        "$85,000 - $105,000 a year -  Full-time\n",
        "\n",
        "Job details\n",
        "Salary\n",
        "$85,000 - $105,000 a year\n",
        "Job Type\n",
        "Full-time\n",
        "Qualifications\n",
        "•\tEducation:\n",
        "•\tBachelor's (Preferred)\n",
        "•\tLocation:\n",
        "•\tSan Antonio, TX 78227 (Required)\n",
        "Benefits\n",
        "Pulled from the full job description\n",
        "Health insurance\n",
        "Dental insurance\n",
        "401(k)\n",
        "Flexible spending account\n",
        "Paid time off\n",
        "Vision insurance\n",
        "401(k) matching\n",
        "Life insurance\n",
        "Full Job Description\n",
        "The Senior Data Analyst assists the Division Director delivering data driven solutions to healthcare and public safety organizations. The candidate turns data into information, information into insight, and insight into actionable deliverables.\n",
        "•\tYou should have demonstrated experience with SQL, MySQL, and PostgreSQL databases including use of server tools. You will be part of a small cross-functional, agile team where you will have continued opportunity to learn and grow.\n",
        "•\tYou will help determine end user requirements, develop project plans, perform data functions, and create reports and visualizations. You will be responsible for system maintenance such as software updates, user management, data standardization, and system performance improvements.\n",
        "•\tWe look for self-starters who thrive in a fast-paced, agile environment – which means wearing many hats, being able to change direction quickly, and showing an eagerness to learn new technologies as the need arises. And most importantly – we look for people who can prioritize, multi-task, and deliver.\n",
        "Responsibilities \n",
        "•\tDevelop and maintain reports, dashboards, and data extraction routine\n",
        "•\tPerform data analysis, data validation, and data mapping/design\n",
        "•\tDevelop database queries, views, and stored procedures to support analytics\n",
        "•\tUse statistical methods to analyze data\n",
        "•\tDetermine user needs and design appropriate analytical solutions\n",
        "•\tAcquire, extract, transform, clean, and filter data\n",
        "•\tDevelop and update technical documentation including database documentation, standards, procedures and data dictionaries\n",
        "•\tMaintain detailed, tiered user accounts insuring proper data access\n",
        "•\tMaintain data analytics platforms and databases\n",
        "•\tProvide technical support and problem resolution for analytics platforms\n",
        "Qualifications \n",
        "· Undergraduate Degree (BA or BS) in Data Analytics, Statistics, or a relevant field from an accredited college or university\n",
        "· Minimum 3 years Tableau Server and Desktop development and maintenance experience required\n",
        "· Minimum 3 years database experience with a solid understanding of SQL, MySQL, and PostgreSQL databases\n",
        "· High level of mathematical and statistical knowledge\n",
        "· Accuracy and consistency when preparing reports\n",
        "· Understanding of database structure and data modeling\n",
        "· Adept at creating queries including quality assurance and optimization\n",
        "· Healthcare or Public Safety experience is highly desired\n",
        "· Ability to work independently with minimal supervision\n",
        "· Ability to work with time constraints and handle stressful situations\n",
        "· Strong attention to detail with sound judgement\n",
        "· Ability to collaborate and communicate effectively\n",
        "•\tAbility to work through ambiguity and deal with shifting priorities\n",
        "•\tAbility to mentor other analysts, as well as report to management.\n",
        "•\tProficiency in documenting processes\n",
        "Work Environment\n",
        "•\tYour choice between Mac or PC\n",
        "•\tWork provided phone\n",
        "•\tWork provided hotspot with unlimited data\n",
        "•\tStocked break room with complimentary snacks and drinks\n",
        "•\t401k with company match\n",
        "•\tWorks in multiple environments. Must possess physical and mental health to meet the demands of the position. Must be able to travel up to 25% and participate in various conferences, regional exercises, the Regional Medical Operations Center, and appropriate meetings related to state, regional emergency management and Acute Healthcare activities. Must be able to serve for extended periods of time, with high stress loads.\n",
        "Competencies \n",
        "•\tAnalytical - Collects / researches data; Uses intuition and experience to supplement data.\n",
        "•\tProblem Solving - Identifies and resolves problems in a timely manner; Gathers and analyzes information skillfully; Develops alternative solutions.\n",
        "•\tProject Management - Develops project plans; Communicates changes and progress; Completes projects on time and within budget.\n",
        "•\tOral Communication - Speaks clearly and persuasively; Listens and gets clarification when needed; Responds well to questions; Contributes thoughts, ideas and suggestions.\n",
        "•\tWritten Communication - Writes clearly and informatively; Edits work; Varies writing style to meet needs; Presents numerical data effectively; Able to read and interpret written information.\n",
        "•\tTeamwork - Balances team and individual responsibilities; Exhibits objectivity and openness to others' views; Gives and solicits feedback.\n",
        "•\tQuality Management - Looks for ways to improve and promote quality; Demonstrates accuracy and thoroughness.\n",
        "•\tBusiness Acumen - Understands business implications of decisions; Demonstrates knowledge of market and competition; Aligns work with strategic goals.\n",
        "•\tJudgment - Exhibits sound and accurate judgment; Supports and explains reasoning for decisions; Includes appropriate people in decision-making process.\n",
        "•\tPlanning/Organizing - Prioritizes and plans work activities; Sets goals and objectives.\n",
        "•\tQuality - Demonstrates accuracy and thoroughness; Looks for ways to improve and promote quality; Applies feedback to improve performance.\n",
        "Code of Conduct: \n",
        "Employee follows the STRAC Code of Conduct, which are rules to guide us in our work to assure the highest standards of business ethics and compliance as follows:\n",
        "1. Legal Compliance: comply with federal/state laws\n",
        "2. Business Ethics: accurately & honestly represent the Organization and not defraud anyone of money, property or service.\n",
        "3. Confidentiality: protect confidential information\n",
        "4. Conflict of Interest: do not use position to profit personally\n",
        "5. Business Relationships: business transactions are free from offers or solicitation of gifts/favors\n",
        "6. Protection of Assets: preserve assets by using resources prudently and effectively\n",
        "7. Patient Rights: respect and support patient rights to privacy & treatment\n",
        "About the Southwest Texas Regional Advisory Council \n",
        "The Southwest Texas Regional Advisory Council (STRAC) is designated by the Texas Department of State Health Services (DSHS) to develop, implement and maintain a regional trauma and emergency healthcare system for the 22 counties in Trauma Service Area P. TSA-P has a mixture of urban, suburban, rural and frontier counties, including the 7th largest city in the United\n",
        "States to the International Border with Mexico. The region encompasses over 26,000 square miles in southwest Texas. STRAC is one of twenty-two regional advisory councils in Texas that comprise the Texas Trauma and Emergency Healthcare System.\n",
        "The Southwest Texas Regional Advisory Council, together with its leadership, members and staff, strives to maintain its reputation as a preeminent leader in regional healthcare coordination within Texas and across the United States. Working collaboratively alongside EMS providers, hospital and healthcare system leadership, as well as local governments, STRAC leverages its consensus based approach to innovative program development that has led to cutting edge improvements in regional emergency healthcare, including its Regional Prehospital\n",
        "Whole Blood Program, Regional Communication and Coordination Center (MEDCOM), Mental\n",
        "Health Patient Navigation System alongside our Law Enforcement Partners and the Texas\n",
        "Emergency Medical Task Force.\n",
        "STRAC is a 501c3 non-profit, tax-exempt member organization consisting of 74 general and specialty hospitals, including 2 Level I Trauma Centers, 16 PCI centers, 12 Stroke centers, air medical providers, and over 70 Fire and EMS agencies.'\n",
        "'\n",
        "Work Location:\n",
        "•\tOne location\n",
        "Job Type: Full-time\n",
        "Pay: $85,000.00 - $105,000.00 per year\n",
        "Benefits:\n",
        "•\t401(k)\n",
        "•\t401(k) matching\n",
        "•\tDental insurance\n",
        "•\tFlexible spending account\n",
        "•\tHealth insurance\n",
        "•\tLife insurance\n",
        "•\tPaid time off\n",
        "•\tVision insurance\n",
        "Schedule:\n",
        "•\t8 hour shift\n",
        "•\tMonday to Friday\n",
        "•\tOn call\n",
        "Education:\n",
        "•\tBachelor's (Preferred)\n",
        "Location:\n",
        "•\tSan Antonio, TX 78227 (Required)\n",
        "Work Location: One location\n",
        "Hiring Insights\n",
        "Hiring 1 candidate for this role\n",
        "Urgently hiring\n",
        "Job activity\n",
        "Employer reviewed job 4 days ago\n",
        "Posted 30+ days ago\n",
        "30+ days ago\n",
        "If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.\n",
        " '''\n"
      ],
      "metadata": {
        "id": "oX5WPZS5kPAF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume1 = ''' Juan Jose Carin \n",
        "Mountain View, CA 94041 650-336-4590 | juanjose.carin@gmail.com Data Scientist linkedin.com/in/juanjosecarin | juanjocarin.github.io \n",
        "\n",
        "Professional Profile \n",
        "Passionate about data analysis and experiments, mainly focused on user behavior, experience, and engagement, with a solid background in data science and statistics, and extensive experience using data insights to drive business growth. \n",
        "Education \n",
        "\n",
        "2016 University of California, Berkeley Master of Information and Data Science \n",
        "• Machine Learning\n",
        "• Machine Learning at Scale \n",
        "• Data Visualization and Communication \n",
        "• Research Design and Applications for \n",
        "GPA: 3.93 \n",
        " \n",
        "Relevant courses: \n",
        "• Field Experiments\n",
        "• Applied Regression and Time Series \n",
        "Analysis \n",
        "• Storing and Retrieving Data \n",
        "• Exploring and Analyzing Data \n",
        "Data Analysis \n",
        "2014 \n",
        "Universidad Politécnica de Madrid M.S. in Statistical and Computational Information Processing \n",
        "• Data Mining\n",
        "• Multivariate Analysis • Time Series \n",
        "• Monte Carlo Techniques\n",
        "• Numerical Methods in Finance • Stochastic Models in Finance • Bayesian Networks \n",
        "GPA: 3.69 \n",
        "Relevant courses: \n",
        "• Neural Networks and Statistical Learning \n",
        "• Regression and Prediction Methods • Optimization Techniques \n",
        "2005 \n",
        "Universidad Politécnica de Madrid M.S. in Telecommunication Engineering \n",
        "GPA: 3.03 \n",
        "Focus Area: Radio communication systems (radar and mobile).\n",
        "Fellowship: First year at University, due to Honors obtained last year at high school. \n",
        "Skills \n",
        "Experience \n",
        "DATA SCIENCE \n",
        "\n",
        "Proficient: Intermediate: Basic: \n",
        "Programming / Statistics R, Python, SQL\n",
        "SPSS, SAS, Matlab EViews, Demetra+ \n",
        "Big Data\n",
        "Hadoop, Hive, MrJob Spark, Storm \n",
        "Visualization \n",
        "Tableau D3.js \n",
        "Others\n",
        "Git, AWS\n",
        "Bash\n",
        "Gephi, Neo4j, QGIS \n",
        "\n",
        "Jan. 2016 – Mar. 2016 \n",
        "Data Scientist \n",
        "CONENTO \n",
        "Madrid, Spain (working remotely) \n",
        "Jun. 2014 – Sep. 2014 \n",
        "• Designed and implemented the ETL pipeline for a predictive model of traffic on the main roads in eastern Spain (a project for the Spanish government). \n",
        "• Automated scripts in R to extract, transform, clean (incl. anomaly detection), and load into MySQL data from multiple data sources: road traffic sensors, accidents, road works, weather. \n",
        "Data Scientist \n",
        "CONENTO \n",
        "Madrid, Spain \n",
        "•\tDesigned an experiment for Google Spain (conducted in October 2014) to measure the impact of YouTube ads on the sales of a car manufacturer's dealer network. \n",
        "•\tA matched-pair, cluster-randomized design, which involved selecting the test and control groups from a sample of 50+ cities in Spain (where geo-targeted ads were possible) based on their sales- wise similarity over time, using wavelets (and R). \n",
        "MANAGEMENT – SALES (Electrical Eng.) \n",
        "\n",
        "Feb. 2009 – Aug. 2013 Head of Sales, Spain & Portugal – Test &Measurement dept. \n",
        "YOKOGAWA \n",
        "Madrid, Spain \n",
        "• Applied analysis of sales and market trends to decide the direction of the department. • Led a team of 7 people. \n",
        "1 of 2 '''\n"
      ],
      "metadata": {
        "id": "6ppauqmgkMCk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume2 = ''' John A Baker Jr.\n",
        "Data Scientist | Machine Learning Engineer\n",
        "San Antonio, TX | (619) 392-1231 | baker.john20@yahoo.com | johnabakerjr.link | GitHub: JohnBaker | LinkedIn: JohnBaker\n",
        "TECHNICAL SKILLS\n",
        " \n",
        "Development: Python (NumPy, Pandas, Scikit-learn), SQL, Tableau, PyTorch, Keras, Spark, Flask, Plotly Dash, Heroku\n",
        "Data Science: TensorFlow, Matplotlib, Data Analysis/Cleaning, Predictive Analytics, Machine Learning, Neural Networks\n",
        "Expertise: Project Management, Business Development, Technical Sales, Real Estate, Digital Marketing, Data Collection\n",
        "PROJECTS\n",
        " \n",
        "Plotly/Dash app, Life Expectancy Animation (1952-2007) - Data Scientist/Programmer, Heroku\t2021\n",
        "Plotly/Dash app, Life Expectancy SunBurst (2007) - Data Scientist/Programmer, Heroku\t2021\n",
        "●\tResearched the life expectancy data from Kaggle to understand and predict how different features affect longevity.\n",
        "●\tDecided on which animation features could be used to produce a fully interactive visualization of the data analysis. ● Built scatter plot apps using Plotly Dash to allow users to see how GDP correlates with life expectancy long-term.\n",
        "Custom Tableau Project, Fully Interactive Business Dashboard - Data Scientist/Programmer, Tableau Public\t2021 ● Researched the stock data that is associated with Tableau to evaluate and understand data for implementation.\n",
        "●\tDecided appropriate resource and tool to use to create a fully functional site that displayed results found.\n",
        "●\tBuilt a dashboard that shows total sales for each region, total profit, total orders,  daily sales, and much more.\n",
        "EXPERIENCE\n",
        " \n",
        "Underdog Devs, Remote - Machine Learning Engineer, Underdogdevs\t2022 - 2022\n",
        "●\tWorked in a team environment to make foundational decisions about the development of the product.\n",
        "●\tBuilt real software for a real cause organization, using Trello and GitHub to track all product development.\n",
        "●\tGained experience in database, machine learning, and visualizations as a collaborative exercise, where team members contributed their skills, experience, and time to collectively create a product.\n",
        "Combat Realty, Chula Vista, CA - Broker/Owner\t2013 - 2020\n",
        "●\tUsed the proprietary information of the company to analyze and collect customer data to increase business.\n",
        "●\tPerformed data integrity on all customers accounts and ensured the data was safe and secure at all times.\n",
        "●\tTrained, and coached a team of 9 to 12 on how to properly store data and stay compliant without data breach.\n",
        "●\tMarketed the residential real estate business via digital/social campaigns, lead generation, and cold calling.\n",
        "●\tPerformed payroll duties weekly for sales associates and salaried employees and corrected any discrepancies.\n",
        "Prudential California Realty, Chula Vista, CA - Salesperson\t2009 - 2013\n",
        "●\tCollected customer data and accurately wrote real estate contracts for sales and leases.\n",
        "●\tFostered a positive work environment with 3 owners, 40+ coworkers, brokers, and salespeople with no complaints.\n",
        "●\tPerformed more than 100-300 cold calls per day and understood the customers housing needs to make the sales.\n",
        "EDUCATION\n",
        " \n",
        "BloomTech (Lambda School), Data Science\t2021 - 2022\n",
        "● Immersive curriculum focused on predictive modeling, data engineering, machine learning, and computer science.\n",
        "San Diego City College, Real Estate'''"
      ],
      "metadata": {
        "id": "kAO1HOiLjYa1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing stop words corpus and lemmatizer from NLTK Library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "wn = nltk.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "6BkNLafttpYQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(data):\n",
        "    remove_numbers = re.sub('[0-9\\n]','',data)\n",
        "\n",
        "    data_lower = remove_numbers.lower()\n",
        "\n",
        "    beg_and_trail_spaces = data_lower.strip()\n",
        "\n",
        "    email_out = re.sub(r'[a-zA-Z0-9_.+-]+@[ a-zA-Z0-9-]+\\.[ a-zA-Z0-9-.]+', '', beg_and_trail_spaces)\n",
        "\n",
        "    txt_nopunct = \"\".join([c for c in email_out if c not in string.punctuation])\n",
        "\n",
        "    tokens = re.split('\\W+', txt_nopunct)\n",
        "\n",
        "    txt_tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "    clean_text = [wn.lemmatize(word) for word in txt_tokens]\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "rOPclnrns69V"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Turning the job description and resume into lemmitized list\n",
        "- First, we compared a bakery job to data science resume 1. <br/>\n",
        "- Second, we compared a bakery job to data science resume 2. <br/>\n",
        "- Third, we compared a data science job to data science resume 1. <br/>\n",
        "- Forth, we compared a data science job to data science resume 2. <br/>\n",
        "- Fifth, we compared the bakery job to the bakery job. <br/>\n",
        "- Sixth, we compared the data science job to the data science job. <br/> "
      ],
      "metadata": {
        "id": "9ZMT84YC-IBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bakery_job_description_clean = cleaner(bakery_job_description)\n",
        "bakery_job_description_clean_str =' '.join(bakery_job_description_clean)\n",
        "\n",
        "data_job_description_clean = cleaner(data_job_description)\n",
        "data_job_description_clean_str =' '.join(data_job_description_clean)\n",
        "\n",
        "resume1_clean = cleaner(resume1)\n",
        "resume1_clean_str =' '.join(resume1_clean)\n",
        "\n",
        "resume2_clean = cleaner(resume2)\n",
        "resume2_clean_str =' '.join(resume2_clean)"
      ],
      "metadata": {
        "id": "AtXe5dpZVUoA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bakery_Data1 = [bakery_job_description_clean_str, resume1_clean_str]\n",
        "bakery_Data2 = [bakery_job_description_clean_str, resume2]\n",
        "data_Data1 = [data_job_description_clean_str, resume1_clean_str]\n",
        "data_Data2 = [data_job_description_clean_str, resume2_clean_str]\n",
        "bakery_bakery = [bakery_job_description_clean_str, bakery_job_description_clean_str]\n",
        "data_Data = [data_job_description_clean_str, data_job_description_clean_str]"
      ],
      "metadata": {
        "id": "xViUMBwOaTFo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We made a Count Vectorizer\n",
        "CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix.\n",
        "### We fit and transformed all the data\n",
        "This method performs fit and transform on the input data at a single time and converts the data points. If we use fit and transform separate when we need both then it will decrease the efficiency of the model so we use fit_transform() which will do both fit and transform."
      ],
      "metadata": {
        "id": "SbN_3UC62425"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()\n",
        "matrix_bakery_Data1 = cv.fit_transform(bakery_Data1)\n",
        "matrix_bakery_Data2 = cv.fit_transform(bakery_Data2)\n",
        "matrix_data_Data1 = cv.fit_transform(data_Data1)\n",
        "matrix_data_Data2 = cv.fit_transform(data_Data2)\n",
        "matrix_bakery_bakery = cv.fit_transform(bakery_bakery)\n",
        "matrix_data_Data = cv.fit_transform(data_Data)"
      ],
      "metadata": {
        "id": "EGwhr6zYVWsI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we take the cosine similarity\n",
        "Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. We are using it to measure document similarity."
      ],
      "metadata": {
        "id": "I_zrqfmp1wvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix_bakery_Data1 = cosine_similarity(matrix_bakery_Data1)\n",
        "similarity_matrix_bakery_Data2 = cosine_similarity(matrix_bakery_Data2)\n",
        "similarity_matrix_data_Data1 = cosine_similarity(matrix_data_Data1)\n",
        "similarity_matrix_data_Data2 = cosine_similarity(matrix_data_Data2)\n",
        "similarity_matrix_bakery_bakery = cosine_similarity(matrix_bakery_bakery)\n",
        "similarity_matrix_data_Data = cosine_similarity(matrix_data_Data)"
      ],
      "metadata": {
        "id": "Dimgsxb9axdI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing the results\n",
        "The results of the bakery job compared to the data science resume were not encouraging. We were hoping to get back a lower score. The results show what we already knew, this is not a perfect way to do it. Although, a score of over 60% when compared to a job you made your resume for, could be a good indicator that you should at least try to get the job."
      ],
      "metadata": {
        "id": "0qmJS87k1lYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Data resume #1 matches baking job By: '+ str(round(similarity_matrix_bakery_Data1[1][0]*100))+ '%')\n",
        "print('Data resume #2 matches baking job By: '+ str(round(similarity_matrix_bakery_Data2[1][0]*100))+ '%')\n",
        "print('Data resume #1 matches data job By: '+ str(round(similarity_matrix_data_Data1[1][0]*100))+ '%')\n",
        "print('Data resume #2 matches data job By: '+ str(round(similarity_matrix_data_Data2[1][0]*100))+ '%')\n",
        "print('Data job matches data job By: '+ str(round(similarity_matrix_bakery_bakery[1][0]*100))+ '%')\n",
        "print('Data job matches data job By: '+ str(round(similarity_matrix_data_Data[1][0]*100))+ '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6fKZc8oaySh",
        "outputId": "8aaecbf9-a6fc-4e40-b47a-c953926114d1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data resume #1 matches baking job By: 4%\n",
            "Data resume #2 matches baking job By: 4%\n",
            "Data resume #1 matches data job By: 35%\n",
            "Data resume #2 matches data job By: 37%\n",
            "Data job matches data job By: 100%\n",
            "Data job matches data job By: 100%\n"
          ]
        }
      ]
    }
  ]
}